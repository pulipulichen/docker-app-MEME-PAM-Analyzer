{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ce579b-4e23-4727-a269-591fe4070d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-31 09:15:30.790141: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-31 09:15:30.811955: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-31 09:15:30.811991: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-31 09:15:30.812734: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-31 09:15:30.816704: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-31 09:15:31.309730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for /home/jovyan/.keras-ocr/craft_mlt_25k.h5\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260: resize_bilinear (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.image.resize(...method=ResizeMethod.BILINEAR...)` instead.\n",
      "Looking for /home/jovyan/.keras-ocr/crnn_kurapan.h5\n"
     ]
    }
   ],
   "source": [
    "# keras-ocr will automatically download pretrained\n",
    "# weights for the detector and recognizer.\n",
    "if 'pipeline' not in globals():\n",
    "    # https://youtu.be/3RNPJbUHZKs\n",
    "    \"\"\"\n",
    "    Remove text from images\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import keras_ocr\n",
    "    from PIL import Image\n",
    "    import cv2\n",
    "    import math\n",
    "    import numpy as np\n",
    "\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "    import glob\n",
    "    import requests\n",
    "    import shutil\n",
    "    \n",
    "    pipeline = keras_ocr.pipeline.Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e807e18f-fc97-4f91-a359-f2b8e114c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#General Approach.....\n",
    "#Use keras OCR to detect text, define a mask around the text, and inpaint the\n",
    "#masked regions to remove the text.\n",
    "#To apply the mask we need to provide the coordinates of the starting and \n",
    "#the ending points of the line, and the thickness of the line\n",
    "\n",
    "#The start point will be the mid-point between the top-left corner and \n",
    "#the bottom-left corner of the box. \n",
    "#the end point will be the mid-point between the top-right corner and the bottom-right corner.\n",
    "#The following function does exactly that.\n",
    "def midpoint(x1, y1, x2, y2):\n",
    "    x_mid = int((x1 + x2)/2)\n",
    "    y_mid = int((y1 + y2)/2)\n",
    "    return (x_mid, y_mid)\n",
    "\n",
    "#Main function that detects text and inpaints. \n",
    "#Inputs are the image path and kreas_ocr pipeline\n",
    "def inpaint_text(img_path, pipeline):\n",
    "    # read the image \n",
    "    img = keras_ocr.tools.read(img_path) \n",
    "    \n",
    "    # Recogize text (and corresponding regions)\n",
    "    # Each list of predictions in prediction_groups is a list of\n",
    "    # (word, box) tuples. \n",
    "    prediction_groups = pipeline.recognize([img])\n",
    "    \n",
    "    #Define the mask for inpainting\n",
    "    mask = np.zeros(img.shape[:2], dtype=\"uint8\")\n",
    "    inpainted_img = False\n",
    "    for box in prediction_groups[0]:\n",
    "        x0, y0 = box[1][0]\n",
    "        x1, y1 = box[1][1] \n",
    "        x2, y2 = box[1][2]\n",
    "        x3, y3 = box[1][3] \n",
    "        \n",
    "        x_mid0, y_mid0 = midpoint(x1, y1, x2, y2)\n",
    "        x_mid1, y_mi1 = midpoint(x0, y0, x3, y3)\n",
    "        \n",
    "        #For the line thickness, we will calculate the length of the line between \n",
    "        #the top-left corner and the bottom-left corner.\n",
    "        thickness = int(math.sqrt( (x2 - x1)**2 + (y2 - y1)**2 ))\n",
    "        \n",
    "        #Define the line and inpaint\n",
    "        cv2.line(mask, (x_mid0, y_mid0), (x_mid1, y_mi1), 255,    \n",
    "        thickness)\n",
    "        inpainted_img = cv2.inpaint(img, mask, 7, cv2.INPAINT_NS)\n",
    "                 \n",
    "    return(inpainted_img)\n",
    "\n",
    "# ================================================\n",
    "\n",
    "def preprocess_visual_inpaint(inputFile):\n",
    "  if os.path.isfile(inputFile) is False:\n",
    "    return False\n",
    "    \n",
    "  if inputFile.endswith('.inpaint.jpg') or inputFile.endswith('.crop.jpg'):\n",
    "    return\n",
    "\n",
    "  # p = Path(inputFile)\n",
    "  # name = p.name\n",
    "  \n",
    "  # outputFile = os.path.splitext(inputFile)[0] + '.inpaint.jpg'\n",
    "  # outputFile2 = os.path.splitext(inputFile)[0] + '.crop.jpg'\n",
    "  outputFile = inputFile + '.inpaint.jpg'\n",
    "  outputFile2 = inputFile + '.crop.jpg'\n",
    "    \n",
    "  # outputFile3 = os.path.splitext(inputFile)[0] + '.inpaint.crop.jpg'\n",
    "  \n",
    "  if os.path.isfile(outputFile) or os.path.isfile(outputFile2):\n",
    "    # print('File is exsited: ' + outputFile + ' / ' + outputFile2)\n",
    "    return False\n",
    "\n",
    "  # print('Processing: ' + inputFile)\n",
    "\n",
    "  img_text_removed = inpaint_text(inputFile, pipeline)\n",
    "  if img_text_removed is False:\n",
    "    shutil.copyfile(inputFile, outputFile)\n",
    "  else:\n",
    "    cv2.imwrite(outputFile, cv2.cvtColor(img_text_removed, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "  return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f335f17-8c0c-4c69-8349-b552cadff5a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'cropper' not in globals():\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "    import glob\n",
    "    import requests\n",
    "    import shutil\n",
    "    from PIL import Image\n",
    "    from autocrop import Cropper\n",
    "    import subprocess\n",
    "    \n",
    "    cropper = Cropper(face_percent=70)\n",
    "\n",
    "\n",
    "# inputFiles = glob.glob(\"/2.output/**/*.inpaint.jpg\", recursive=True)\n",
    "# for inputFile in inputFiles:\n",
    "def preprocess_visual_crop(inputFile):\n",
    "  if os.path.isfile(inputFile) is False:\n",
    "    return False\n",
    "    \n",
    "  if inputFile.endswith('.inpaint.jpg') is False:\n",
    "    return False\n",
    "\n",
    "  p = Path(inputFile)\n",
    "  name = p.name\n",
    "  outputFile = inputFile[:-12] + '.crop.jpg'\n",
    "  \n",
    "  if os.path.isfile(outputFile):\n",
    "    # print('File is exsited: ' + outputFile)\n",
    "    os.remove(inputFile)\n",
    "    return\n",
    "\n",
    "  # print('Processing: ' + inputFile + ' => ' + outputFile)\n",
    "\n",
    "  image = cv2.imread(inputFile)\n",
    "  height, width, channels = image.shape\n",
    "  if width < 300:\n",
    "    image = cv2.resize(image, (600, 600), interpolation=cv2.INTER_LINEAR)\n",
    "    cv2.imwrite('/tmp/tmp.jpg', image)\n",
    "    inputFile = '/tmp/tmp.jpg'\n",
    "  \n",
    "  cropped_array = cropper.crop(inputFile)\n",
    "\n",
    "  # print(cropped_array)\n",
    "  # Save the cropped image with PIL if a face was detected:\n",
    "  if cropped_array is None:\n",
    "    #shutil.copyfile(inputFile, outputFile)\n",
    "    subprocess.run([\"smartcrop\",\"-W\",\"256\",\"-H\",\"256\", \"-i\",inputFile, \"-o\",outputFile])\n",
    "  else:\n",
    "    # cropped_image = Image.fromarray(cropped_array)\n",
    "    #cropped_image.save(outputFile)\n",
    "    cv2.imwrite(outputFile, cv2.cvtColor(cropped_array, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "  os.remove(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d71710c-553f-4f84-b444-f150170c3551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def preprocess_visual(image_path):\n",
    "    \n",
    "    print(\"[preprocess_visual] \", image_path ,  datetime.datetime.now())\n",
    "    \n",
    "    preprocess_visual_inpaint(image_path)\n",
    "    preprocess_visual_crop(image_path + '.inpaint.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485e790e-ae1c-4936-a06f-659f23e03687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_visual_crop(\"./input/data/a/10th-Zoom-Meeting.avif.inpaint.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "871f34b6-cf3d-4ebe-b802-0d58e02e4d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !smartcrop --help"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
